## 2020.5.26-27CPU上下文切换

### 内容
1. 线程和进程的上下文以及何时触发调度。
2. CPU上下文切换实验。

### 线程和进程的上下文以及调度
1. 线程的上下文，运行堆栈和寄存器。
2. 进程的上下文，进程不是调度实体，但是为调度实体线程提供了运行所需的堆栈内存。其中堆栈内存都保存在虚拟内存中，虚拟内存通过MMU（在内核的内存中？）映射到物理内存，TLB又对MMU进行缓存，因此进程切换会导致TLB缓存内容替换。
3. 线程调用系统调用时，触发2次线程调度，其中只需要保存线程运行的上下文，并且在系统调用结束后恢复。
4. 线程运行结束（1.阻塞。2.运行时间片到期。等原因），结束后会调度运行另一个线程。此时有2种情况，1是这个线程和上一个线程于同一个进程，2是这2个线程属于不同进程。这2种情况的共同点是都需要做线程上下文切换。不同点是TLB中缓存的MMU内存对情况2无效。
5. 中断。中断的本质是信号？信号处理会导致正在运行的进程被发送信号的进程抢占，从而发生4所描述的不同进程的线程切换？（以上纯属猜测，需要验证）
6. 思考：Go协程相比于线程优势在哪里？1.内存大小优势。协程的初始栈远比物理线程小。2.创建和销毁的成本。线程的创建和销毁都需要内核的系统调用，协程只需要用户态代码。3.调度成本，阻塞等操作触发调度时，切换协程只需要修改3个寄存器的值，而线程需要陷入内核态切换运行堆栈和寄存器等上下文。

### sysbench实验
1. 实验描述。用不同sysbench执行10W个请求，每次使用的线程数不同，通过vmstat查看上下文切换次数，top查看CPU使用率，以及sysbench测试结果的平均时间。测试机器为8核，Debian。命令：sysbench  --test=threads --max-requests=100000 --num-threads=N run。
2. 实验结果
   
   | thread数量 | 中断次数 | 上下文切换次数 | top：CPU使用率 | 总耗时 | 平均耗时 |
   | ---- | ---- | ---- | ---- | ---- | ---- |
   | N = 600 | 18W | 260W |570% | 33S | 192ms |
   | N = 60 | 15-20W | 150-250W | 480-570% | 35S | 21ms |
   | N = 6 | 3000 | 5-20K | 600% | 9S | 0.58ms |
3. 结果分析：a.频繁的上下文切换会导致处理速度下降。不过载时只需9S，过载时需要30S左右。b.过载会引起系统的雪崩，任务多，频繁切换上下文，处理速度下降，导致累计的任务又进一步增多，最终雪崩。
4. 存在疑惑的点。总的耗时和平均耗时不成正比，平均耗时下降的很快，但总的耗时下降很少，那么任务执行让出的时间在哪里被占用了呢？（需要了解sysbench执行任务的方式和统计的方式）.目前猜测的计算方式：任务开始时t1，任务结束时t2，t2-t1为耗时，这个包括了上下文切换。total =  avg * req_num / thread_num。N=600时，192*10W / 600 = 32S。 N=60时，21*10W / 60 = 35S。N=6时，0.58*10W/6 = 9.5S。N=60和N=600时上下文切换次数相近，所以总的运行时间也相近。
5. sysbench的作用。模拟CPU, thread, I/O, Mysql, Memory等操作，测试机器的性能和做实验。